---
title: "Final Data Project"
date: '2022-07-26'
output:
  html_document: 
    theme: readable
    toc: yes
  pdf_document: default
  urlcolor: cyan
  params:
    hardcore: FALSE
---
***    
# Instructions DELETE BEFORE SUBMISSION

[Project Instructions Page](https://d18ky98rnyall9.cloudfront.net/IAZ-yE1WQPWGfshNVoD1SQ_9a94df6ec90a4643aad57503dfa2a4f1_data-project-su22.html?Expires=1659225600&Signature=eLrwlUo9dqDJC3rmDl2CYqyfn1U-MreTF6Wgy7L5TfT6QfJHweAbSitGHvWi-XWmMx7b3m9EMCvATJkFvRBVyEgPExCRwB8OUQhhJrQtOyI2P437ms3fLT1aqcOoBxoR8tgJ84FYYXQexOvDURrboIn7CYkHTjCqCcK79tUiZMU_&Key-Pair-Id=APKAJLTNE6QMUY6HBC5A)

[Project Instruction Video](https://mediaspace.illinois.edu/media/0_q1hn44wt)  


**Notes from Video instructions**

- Expectation is that we explore the data. Introduction lays out a much more robust background.
- Methods section 1 is where we introduce some exploratory analysis, explore correlation analysis, scatterplots, colinearity, explore.
- Methods section 2 is where we take our exploratory analysis then build some models and maybe we stop at additive, maybe we go from additive to interactive. analyze RMSE, adj.r.squared, residual std error.
-No matter what we then look for how to improve our additive model by shrinking the model intelligently or growing the model.  

- choose a couple of models into a table  
-- model, AIC/BIC, number of variables, RMSE, some stats

**The goal of the project is to explore the data, not make the best model**  
The best way to fail is to make a path and just stick to it. Don't put the blinders on.


***

```{r, setup, echo = FALSE, message = FALSE, warning = FALSE}
options(scipen = 1, digits = 4, width = 80, fig.align = "center")

#This condition controls whether eval is set to true or false for more time consuming knits.
hardcore = TRUE
```

# Introduction
The data we have chosen to look at is Housing Prices in California. This data comes from [Kaggle](https://www.kaggle.com/datasets/fedesoriano/california-housing-prices-data-extra-features) and outlines data that would go in to predicting the price of a house in California. As people who currently rent (and one of us living in California), we hope to one day be able to purchase a home and being able to understand this model could help us determine important factors in predicting the price and whether future ones we intend to buy are a good deal or not. 

In this document we will modify and assess the data, then using our asssessments attempt to build a good model which does not overfit or underfit the data. We hope to be able to create a general model that based on some factors about a given property can give an expected price as an output. 


Original Variables:  

1. `Median_House_Value`: Median house value for households within a block (measured in US Dollars) [\$]  

2. `Median_Income`: Median income for households within a block of houses (measured in tens of thousands of US Dollars) [10k\$]  

3. `Median_Age`: Median age of a house within a block; a lower number is a newer building [years]  

4. `Total_Rooms`: Total number of rooms within a block  

5. `Total_Bedrooms`: Total number of bedrooms within a block  

6. `Population`: Total number of people residing within a block  

7. `Households`: Total number of households, a group of people residing within a home unit,
for a block  

8. `Latitude`: A measure of how far north a house is; a higher value is farther north [°]  

9. `Longitude`: A measure of how far west a house is; a higher value is farther west [°]  

10. `Distance_to_coast`: Distance to the nearest coast point [m]  

11. `Distance_to_Los_Angeles`: Distance to the center of Los Angeles [m]  

12. `Distance_to_San_Diego`: Distance to the center of San Diego [m]  

13. `Distance_to_San_Jose`: Distance to the center of San Jose [m]  

14. `Distance_to_San_Francisco`: Distance to the center of San Francisco [m]  

Variables added by the group later in the project:  

15. `dist_to_nearest_city`: The numeric minimum value of variables 11 through 14 divided by 1000 to convert to km. [km] 

16. `nearest_city`: Categorical variable indicating which city of those listed in variable 11 through 14 was the closest.[city name]  

17. `near_a_city_100`: a factor variable indicating 1 if a house is within 100 km of the nearest city or not. [0,1]

18. `near_a_city_200`: a factor variable indicating 1 if a house is within 200 km of the nearest city or not. [0,1]

***
 
# Methods

## Data Analysis and Refinement
### Read-in
First we need to load in the data and prepare some of the columns. 

```{r, readin}
library(readr)
housing_data =  read.csv("California_Houses.csv")
```


To augment the data a bit, we need to take the predictors `Distance to_Los Angeles`, `Distance to Los Angeles`, `Distance to Los Angeles`, and `Distance to Los Angeles` and convert them into three separate columns:
- a factor variable `nearest_city`- This segments the data into regions of California and if there is any relevance in being closer to one city vs. another. 
- a numeric variable `dist_to_nearest_city` - This will give us the distance to this nearest city in kms. 

### Geographic Modifications and Assessment

```{r}
nearest_city = rep("", nrow(housing_data))
dist_to_nearest = rep(0, nrow(housing_data))
near_city = rep(0, nrow(housing_data))


nearest_city_options = c("LA", "San Diego", "San Jose", "San Fransisco")

for (i in 1:nrow(housing_data)) {
 subset = housing_data[i,
                       c("Distance_to_LA", 
                         "Distance_to_SanDiego", 
                         "Distance_to_SanJose", 
                         "Distance_to_SanFrancisco")]
 
 nearest_city[i] = nearest_city_options[which.min(subset)]
 dist_to_nearest[i] = min(subset) / 1000
 # near_city[i] = ifelse(dist_to_nearest[i] < 100, 1, 0)
}

housing_data$nearest_city = as.factor(nearest_city)
housing_data$dist_to_nearest_city = dist_to_nearest
# housing_data$near_a_city = as.factor(near_city)
```


We will then perform a quick assessment of the variables we just created.
First we will inspect what percentage of the properties is closest to each city

```{r, nearest-city-table}
data.frame(
Los_Angeles = mean(housing_data$nearest_city == "LA"),
San_Diego = mean(housing_data$nearest_city == "San Diego"),
San_Jose = mean(housing_data$nearest_city == "San Jose"),
San_Fransisco = mean(housing_data$nearest_city == "San Fransisco"))
```

And overall numbers for nearest city:  
```{r, summary1}
summary(housing_data$nearest_city)
```

We will also run a quick sanity check that ensures that based on latitude and longitude we do indeed have the correct nearest city.

```{r, plot-map}
plot(Latitude ~ Longitude, housing_data,
     col = nearest_city,
     pch = as.numeric(nearest_city))
legend("topright",
       legend = levels(housing_data$nearest_city),
       col = c(1:4),
       pch = c(1:4)) 
```  

Next we will gather some data on how far data points are from the nearest city.  
```{r, summary2}
summary(housing_data$dist_to_nearest_city)
```

And we will plot the distribution of distance as both a box plot and a histogram for help framing these distances.

```{r, nearest-city-plots}
par(mfrow = c(1,2))
boxplot(housing_data$dist_to_nearest_city,
        ylab = "Distance [km]",
        main = "Boxplot of Distances\n to Nearest City")

hist(housing_data$dist_to_nearest_city,
     xlab = "Distance [km]",
     main = "Histogram of Distances\n to Nearest City")
```  



Between the summary information and the boxplot, we can assess that more than 3/4  of the properties in the dataset are within 100 km of the nearest city. We will use this to create a new variable called `near_city_100` a factor variable which evaluates to 1 if within 100 km of a city and 0 otherwise. 

```{r, create-nearest-city-100}
housing_data$near_a_city_100 = as.factor(housing_data$dist_to_nearest_city < 100)

```

We also see that around the 200km mark is where the whisker ends on our boxplot above so we will investigate what proportion of data falls within 200 km

```{r, assess-200-dist}
mean(housing_data$dist_to_nearest_city < 200)
```

given that more than 91% of the data falls within 200 km of a city we will also create another factor variable for this value. 

```{r}
housing_data$near_a_city_200 = as.factor(housing_data$dist_to_nearest_city < 200)
```


Our hope here is that one of the two factor variables created will be a sufficient demarcation line to where certain variables start to have differing effects on the response when we build a model. We will explore that further later.

```{r, near-a-city-assessment}
mean(as.numeric(housing_data$near_a_city_100) - 1)
```
76.64% percent of data points are within  100 km's of the center of the nearest city.

```{r}
mean(as.numeric(housing_data$near_a_city_200) - 1)
```
91.61% percent of data points are within  200 km's of the center of the nearest city.




Having harvested the data from the distances to each city variable we will now eliminate them from the dataset in order to make plotting and analysis more manageable.  
```{r, eliminate-cities}
housing_data = subset(housing_data, 
                      select = -c(Distance_to_LA,
                                  Distance_to_SanFrancisco,
                                  Distance_to_SanDiego,
                                  Distance_to_SanJose))
```

```{r, listofnames}
data.frame(name = names(housing_data))

``` 

### General Data Analysis

With the data loaded and prepped we want to start building the model. Before we do that, we want to check the pairs of all the different predictor variables to see what predictors have strong correlations.
We will leave out
- `Latitude`
- `Longitude`
And represent `nearest_city` as a color and `near_a_city_100` by symbol.

```{r, plotem-all, eval=hardcore}
plot(housing_data[ , c(2:7, 10,12)],
     col = as.numeric(housing_data$nearest_city),
     main = "Plot of Every Variable vs Every Other Variable in Housing Data (some withheld)")
```    


A couple of obvious colinearities jump out. To better see that numerically, we will use the `cor` function from the `faraway`

```{r}
round(cor(housing_data[ , c(2:10,12)]), 2)
```

Below are some of the variables with strong correlation

- `Tot_Rooms` - `Tot_Bedrooms`  
- `Tot_Rooms` - `Population`  
- `Tot_Rooms` - `Households`  
- `Tot_Bedrooms` - `Population`  
- `Tot_Bedrooms` - `Households`  
- `Population` - `Household`

It appears that the variables that have to do with density see a strong positive correlation. This makes sense. As the total number of people within a block (`population`) increases, you also see an increase in total households within a block (`Households`) which leads to an increase in both total bedrooms (`Total_Bedrooms`) and total rooms (`Total_Rooms`). We are not attempting to demonstrate causation, simply how density indicators are linked to each other.

These variables may still have interactions that we will explore later. For instance an area where the population density is low and the number of rooms is high or the number of households is low but the number of rooms is high may indicate an increase in house value. We will keep this in mind for later.




We will also explore variable correlations within the context of whether they are close to or far away from the city, in order to check to see if any patterns emerge within either that were otherwise hidden.

```{r, plotem-city-notcity, eval = hardcore}
str(housing_data)
city = housing_data$near_a_city_100 == TRUE
non_city = ! city
plot(housing_data[ city, c(2:7, 10,12)],
     main = "City Based Plots",
     col = "darkgray")

plot(housing_data[ non_city, c(2:7, 10,12)],
     main = "Non-City Based Plots",
     col = "darkblue")
```

No further trends obviously emerge from breaking out the data into city vs non-city. 

Before we move on from this we will attempt to see if any of the cities have colinearities specific to their locality. In order to do this we will repeat the previous step one for each city using only the city data, in hopes of isolating information specific to the major cities which cover most of the data.

```{r, city-individual-plots, eval = hardcore}
levels(housing_data$nearest_city)
la = housing_data$nearest_city == "LA" & city
sd = housing_data$nearest_city == "San Diego" & city
sf = housing_data$nearest_city == "San Fransisco" & city
sj = housing_data$nearest_city == "San Jose" & city

plot(housing_data[la, c(2:7, 10,12)],
     main = "Los Angeles Based Plots",
     col = 1)

plot(housing_data[sd, c(2:7, 10,12)],
     main = "San Diego Based Plots",
     col = 2)

plot(housing_data[sf, c(2:7, 10,12)],
     main = "San Fransisco Based Plots",
     col = 3)

plot(housing_data[sj, c(2:7, 10,12)],
     main = "San Jose Based Plots",
     col = 4)

```


The last graph we will create for assistance is a graph of `Median_House_Value` vs all other predictors.
We will leave the graphs somewhat sparse to allow for 

```{r}
par(mfrow = c(3,3))
plot(Median_House_Value ~ Median_Income, housing_data, col = 1)
plot(Median_House_Value ~ Median_Age, housing_data, col = 2)
plot(Median_House_Value ~ Tot_Rooms, housing_data, col = 3)
plot(Median_House_Value ~ Tot_Bedrooms, housing_data, col = 4)
plot(Median_House_Value ~ Population, housing_data, col = 5)
plot(Median_House_Value ~ Households, housing_data, col = 6)
plot(Median_House_Value ~ Latitude, housing_data, col = 7)
plot(Median_House_Value ~ Distance_to_coast, housing_data, col = 8)
plot(Median_House_Value ~ dist_to_nearest_city, housing_data, col = 9)
```

## Model Creation and Refinement
To start things off, we are creating a few helper functions for helping us build and evaluate the models we create

```{r, make-functions, echo = FALSE, message = FALSE, warning = FALSE}
library(lmtest)
plot_fitted_resid = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  plot(fitted(model), resid(model), 
       col = pointcol, pch = 20, cex = 1.5,
       xlab = "Fitted", ylab = "Residuals")
  abline(h = 0, col = linecol, lwd = 2)
}

plot_qq = function(model, pointcol = "dodgerblue", linecol = "darkorange") {
  qqnorm(resid(model), col = pointcol, pch = 20, cex = 1.5)
  qqline(resid(model), col = linecol, lwd = 2)
}

get_bp_decision = function(model, alpha) {
  decide = unname(bptest(model)$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_sw_decision = function(model, alpha) {
  decide = unname(shapiro.test(resid(model))$p.value < alpha)
  ifelse(decide, "Reject", "Fail to Reject")
}

get_num_params = function(model) {
  length(coef(model))
}

get_loocv_rmse = function(model) {
  sqrt(mean((resid(model) / (1 - hatvalues(model))) ^ 2))
}

get_adj_r2 = function(model) {
  summary(model)$adj.r.squared
}
```


Now we want to get in to actually creating the model and choosing the best variables for prediction.Before we create any models however, we want to create a train / test set to use for evaluation later.

```{r, split-train-test}
set.seed(420)
housing_data_idx  = sample(nrow(housing_data), size = trunc(0.80 * nrow(housing_data)))
housing_data_trn = housing_data[housing_data_idx, ]
housing_data_tst = housing_data[-housing_data_idx, ]
```

First we will create a simple additive model.

```{r, model-1}
add_model = lm(Median_House_Value ~ ., data = housing_data_trn)
summary(add_model)
```

Here we can see that all of the terms are important to building the model with a low `Pr(>|t|)`. We want to test the VIF with this model

```{r, vif-1}
library(faraway)
summary(add_model)
vif(add_model)
```

Here we can see a couple variables (Tot_Bedrooms, Households, Latitude, and Longitude) all have slightly high VIF. If we perform an F-test with a model with these removed we can check if these are important to keep.

```{r, model-2}
small_mod = lm(Median_House_Value ~ . - Tot_Bedrooms - Households - Latitude - Longitude, data = housing_data_trn)
anova(small_mod, add_model)
```

Here we can see the p-value is <2e-16, so we would reject the smaller model and would say these variables are important. With some of these variables we would assume their interaction with others would be important, say number of bedrooms to the distance to city center. So here we create an interaction model between all of the variables. 

```{r, interaction-model}
int_mod = lm(Median_House_Value ~ . ^ 2, data = housing_data_trn)
get_num_params(int_mod)
```


## Model Comparison and Selection
With the model we created above, we want to step through the parameters to see what's useful to use in our final model. We below we try backward and forward AIC to compare model size and later on LOOCV-RMSE.
```{r, backward-parameter-selection, eval = hardcore}
back_mod_aic = step(int_mod, direction = "backward", trace = 0)
```


```{r,  forward-parameter-selection, eval = hardcore}
mod = lm(Median_House_Value ~ 1, data = housing_data_trn)
for_mod_aic = step(
  mod,
  scope = Median_House_Value ~ (Median_Income + Median_Age + Tot_Rooms + Tot_Bedrooms + Population + Households + Latitude + Longitude + Distance_to_coast +  nearest_city + dist_to_nearest_city + near_a_city_100 + near_a_city_200) ^ 2,
  direction = "forward",
  trace = 0)
```



# Results
We now want to compare RMSE, $R^2$, and percent error of the different models we created.

We did the forward and backward search of the interaction model above. So now we want to check num of parameters in the model

```{r, eval = hardcore}
get_num_params(back_mod_aic)
get_num_params(for_mod_aic)
```

Here we can see our forward model has less parameters than the backward search, so we want to check RMSE for both.
```{r, eval = hardcore}
get_loocv_rmse(back_mod_aic)
get_loocv_rmse(for_mod_aic)
```

The LOOCV-RMSE isn't significantly better, only `r (60487 - 60650) / 60487`% difference. So we would want the model with the lower complexity. We also ant to check RMSE for Train vs. Test for the 3 models (simple additive, interactive, step search model)

```{r,RMSE, eval=hardcore}
rmse = function(actual, predicted) {
  sqrt(mean((actual - predicted) ^ 2))
}

rnames = c("Simple", "Interactive", "Step-Search")
cnames = c("Train", "Test")
results = matrix(rep(0, 6), nrow=3, dimnames = list(rnames, cnames))

results[1, 1] = rmse(housing_data_trn[,"Median_House_Value"], predict(add_model, newdata = housing_data_trn))
results[2, 1] = rmse(housing_data_trn[,"Median_House_Value"], predict(int_mod, newdata = housing_data_trn))
results[3, 1] = rmse(housing_data_trn[,"Median_House_Value"], predict(for_mod_aic, newdata = housing_data_trn))
results[1, 2] = rmse(housing_data_tst[,"Median_House_Value"], predict(add_model, newdata = housing_data_tst))
results[2, 2] = rmse(housing_data_tst[,"Median_House_Value"], predict(int_mod, newdata = housing_data_tst))
results[3, 2] = rmse(housing_data_tst[,"Median_House_Value"], predict(for_mod_aic, newdata = housing_data_tst))

kable(results)
```

We now want to see how good our different models are at predicting data and each of their Adjusted $R^2$.

```{r}

library(knitr)
library(scales)
pred_simp = predict(add_model, newdata = housing_data_tst)
pred_int = predict(int_mod, newdata = housing_data_tst)
pred_chos = predict(for_mod_aic, newdata = housing_data_tst)
act = housing_data_tst[,"Median_House_Value"]

n = nrow(housing_data_tst)

rnames = c("Simple", "Interactive", "Step-Search")
cnames = c("Percent Error", "Adjusted R Squared")
results = matrix(rep(0, 6), nrow=3, dimnames = list(rnames, cnames))

results[1, 1] = (1/n) * sum( abs(pred_simp - act) / pred_simp ) * 100
results[1, 2] = summary(add_model)$adj.r.squared
results[2, 1] = (1/n) * sum( abs(pred_int - act) / pred_int ) * 100
results[2, 2] = summary(int_mod)$adj.r.squared
results[3, 1] = (1/n) * sum( abs(pred_chos - act) / pred_chos ) * 100
results[3, 2] = summary(for_mod_aic)$adj.r.squared

kable(results)

# Graph
opacity = .5
point_size = .3

plot(act, pred_simp, xlab = "Actual Prices", ylab = "Predicted Prices", col=alpha("dodgerblue", opacity), pch = 16, cex = point_size, main = "Actual vs. Predicted Home Prices")
points(act, pred_int, col=alpha(2, opacity), pch = 16, cex = point_size)
points(act, pred_chos, col=alpha(3, opacity), pch = 16, cex = point_size)
abline(a = 0, b = 1, col = "darkorange", lwd = 3)
legend("topleft", c("Simple Additive", "Interactive (Full)", "Interactive (AIC)"), col = c("dodgerblue", 2, 3), pch = 16)
```



*** 

# Discussion

*** 

# Appendix

***
 
#### Group Members
* Brayden Turner - brturne2 
* Caleb Cimmarrusti - Calebtc2
